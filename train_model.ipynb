{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP5Jr8b4/w8Fl3iYC85Lmtu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giangkarry/Machine-Learning/blob/main/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E4cq3hzxw8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c29e622-7005-40d6-88fa-3dbe174e1eb9"
      },
      "source": [
        "!git clone https://github.com/giangkarry/Machine-Learning.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Machine-Learning'...\n",
            "remote: Enumerating objects: 8329, done.\u001b[K\n",
            "remote: Counting objects: 100% (8329/8329), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8327/8327), done.\u001b[K\n",
            "remote: Total 8329 (delta 2), reused 8329 (delta 2), pack-reused 0\n",
            "Receiving objects: 100% (8329/8329), 488.02 MiB | 29.70 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "Checking out files: 100% (8328/8328), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqL9dORpju8U"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "819esVSdj3ei"
      },
      "source": [
        "classes = ('2C', '3C', '4C')\n",
        "TrainTest = namedtuple('TrainTest', ['train', 'test'])\n",
        "traindir = '/content/Machine-Learning/DATA_CHAMBER_2021/train'\n",
        "testdir = '/content/Machine-Learning/DATA_CHAMBER_2021/test'\n",
        "#hàm chuẩn bị dữ liệu\n",
        "def prepare_data():\n",
        "  input_size = 32\n",
        "  transform = transforms.Compose([\n",
        "        transforms.Resize((input_size,input_size)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "  trainset = torchvision.datasets.ImageFolder(root= traindir, transform=transform)\n",
        "  testset  = torchvision.datasets.ImageFolder(root=testdir, transform=transform)\n",
        "\n",
        "  return TrainTest(\n",
        "      train=trainset,\n",
        "      test=testset\n",
        "  )\n",
        "\n",
        "#hàm chuẩn bị các batch để đưa vào lúc train\n",
        "def prepare_loader(datasets):\n",
        "    batch_size = 32\n",
        "    num_workers = 4\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        dataset=datasets.train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        dataset=datasets.test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    return TrainTest(\n",
        "        train=trainloader,\n",
        "        test=testloader\n",
        "    )\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmrBlkCDj3v-"
      },
      "source": [
        "#train\n",
        "def get_trainer(model):\n",
        "  loss = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "  return loss, optimizer\n",
        "\n",
        "#train trong mỗi epoch\n",
        "def train_epoch(epoch, model, loader, loss_func, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    reporting_step = 32\n",
        "    for i, (images, labels) in enumerate(loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if i % reporting_step == reporting_step-1:\n",
        "            print(f\"Epoch {epoch} Step {i} ave_loss {running_loss/reporting_step:0.4f}\")\n",
        "            running_loss = 0.0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI5rAWo3j38G"
      },
      "source": [
        "def test_epoch(epoch, model, loader, device):\n",
        "    model.eval()\n",
        "    ypred = []\n",
        "    ytrue = []\n",
        "    for i, (images, labels) in enumerate(loader):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, dim=1)\n",
        "      ypred += list(predicted.cpu().numpy())\n",
        "      ytrue += list(labels.cpu().numpy())\n",
        "    return ypred, ytrue"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThlVZYxskil4"
      },
      "source": [
        "def main(model = 'vgg16'):\n",
        "  datasets = prepare_data()\n",
        "  loaders = prepare_loader(datasets)\n",
        "  print(\"Tập train: \", len(datasets.train))\n",
        "  print(\"Tập test: \", len(datasets.test))\n",
        "  print(\"class: \", datasets.test.class_to_idx)\n",
        "  \n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  if model == 'vgg16':\n",
        "    model = torchvision.models.vgg16()\n",
        "    model.classifier[-1] = torch.nn.modules.linear.Linear(in_features=4096, out_features=3)\n",
        "  \n",
        "  elif (model == 'vgg19'):\n",
        "    model = torchvision.models.vgg19()\n",
        "    model.classifier[-1] = torch.nn.Linear(in_features=4096, out_features=3)\n",
        "  \n",
        "  elif model == 'resnet50':\n",
        "    model = torchvision.models.resnet50()\n",
        "    model.fc = torch.nn.Linear(in_features=2048, out_features=3)\n",
        "\n",
        "  n_epoch = 10\n",
        "  model.to(device)\n",
        "  print(device)\n",
        "\n",
        "  loss, optimizer = get_trainer(model)\n",
        "  for epoch in range(n_epoch):\n",
        "        train_epoch(epoch, model, loaders.train, loss, optimizer, device)\n",
        "        ypred, ytrue = test_epoch(epoch, model, loaders.test, device)\n",
        "        print(classification_report(ytrue, ypred, target_names=classes))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4VjxzgTlQPd",
        "outputId": "4a60696d-aeb6-4e51-e9f5-441b7edb2aca"
      },
      "source": [
        "main('vgg16')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tập train:  6717\n",
            "Tập test:  1607\n",
            "class:  {'2C': 0, '3C': 1, '4C': 2}\n",
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Step 31 ave_loss 1.0976\n",
            "Epoch 0 Step 63 ave_loss 1.0930\n",
            "Epoch 0 Step 95 ave_loss 1.0120\n",
            "Epoch 0 Step 127 ave_loss 0.7726\n",
            "Epoch 0 Step 159 ave_loss 0.6821\n",
            "Epoch 0 Step 191 ave_loss 0.4376\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.63      0.80      0.71       409\n",
            "          3C       0.75      0.83      0.79       367\n",
            "          4C       0.99      0.81      0.89       831\n",
            "\n",
            "    accuracy                           0.81      1607\n",
            "   macro avg       0.79      0.82      0.80      1607\n",
            "weighted avg       0.84      0.81      0.82      1607\n",
            "\n",
            "Epoch 1 Step 31 ave_loss 0.3513\n",
            "Epoch 1 Step 63 ave_loss 0.2323\n",
            "Epoch 1 Step 95 ave_loss 0.1642\n",
            "Epoch 1 Step 127 ave_loss 0.2488\n",
            "Epoch 1 Step 159 ave_loss 0.1586\n",
            "Epoch 1 Step 191 ave_loss 0.1955\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.61      0.86      0.72       409\n",
            "          3C       0.82      0.94      0.88       367\n",
            "          4C       1.00      0.73      0.84       831\n",
            "\n",
            "    accuracy                           0.81      1607\n",
            "   macro avg       0.81      0.84      0.81      1607\n",
            "weighted avg       0.86      0.81      0.82      1607\n",
            "\n",
            "Epoch 2 Step 31 ave_loss 0.0782\n",
            "Epoch 2 Step 63 ave_loss 0.0551\n",
            "Epoch 2 Step 95 ave_loss 0.0268\n",
            "Epoch 2 Step 127 ave_loss 0.0263\n",
            "Epoch 2 Step 159 ave_loss 0.0495\n",
            "Epoch 2 Step 191 ave_loss 0.0492\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.82      0.82      0.82       409\n",
            "          3C       0.63      0.94      0.75       367\n",
            "          4C       0.99      0.77      0.87       831\n",
            "\n",
            "    accuracy                           0.82      1607\n",
            "   macro avg       0.81      0.85      0.81      1607\n",
            "weighted avg       0.87      0.82      0.83      1607\n",
            "\n",
            "Epoch 3 Step 31 ave_loss 0.0304\n",
            "Epoch 3 Step 63 ave_loss 0.0913\n",
            "Epoch 3 Step 95 ave_loss 0.0029\n",
            "Epoch 3 Step 127 ave_loss 0.0038\n",
            "Epoch 3 Step 159 ave_loss 0.0576\n",
            "Epoch 3 Step 191 ave_loss 0.1001\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.84      0.88      0.86       409\n",
            "          3C       0.70      0.93      0.80       367\n",
            "          4C       1.00      0.83      0.90       831\n",
            "\n",
            "    accuracy                           0.87      1607\n",
            "   macro avg       0.85      0.88      0.86      1607\n",
            "weighted avg       0.89      0.87      0.87      1607\n",
            "\n",
            "Epoch 4 Step 31 ave_loss 0.0078\n",
            "Epoch 4 Step 63 ave_loss 0.0135\n",
            "Epoch 4 Step 95 ave_loss 0.0145\n",
            "Epoch 4 Step 127 ave_loss 0.0055\n",
            "Epoch 4 Step 159 ave_loss 0.0053\n",
            "Epoch 4 Step 191 ave_loss 0.0013\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.56      0.88      0.69       409\n",
            "          3C       0.85      0.87      0.86       367\n",
            "          4C       1.00      0.72      0.83       831\n",
            "\n",
            "    accuracy                           0.79      1607\n",
            "   macro avg       0.80      0.82      0.79      1607\n",
            "weighted avg       0.85      0.79      0.80      1607\n",
            "\n",
            "Epoch 5 Step 31 ave_loss 0.0002\n",
            "Epoch 5 Step 63 ave_loss 0.0001\n",
            "Epoch 5 Step 95 ave_loss 0.0001\n",
            "Epoch 5 Step 127 ave_loss 0.0000\n",
            "Epoch 5 Step 159 ave_loss 0.0001\n",
            "Epoch 5 Step 191 ave_loss 0.0001\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.59      0.86      0.70       409\n",
            "          3C       0.81      0.91      0.86       367\n",
            "          4C       1.00      0.73      0.84       831\n",
            "\n",
            "    accuracy                           0.80      1607\n",
            "   macro avg       0.80      0.83      0.80      1607\n",
            "weighted avg       0.85      0.80      0.81      1607\n",
            "\n",
            "Epoch 6 Step 31 ave_loss 0.0000\n",
            "Epoch 6 Step 63 ave_loss 0.0001\n",
            "Epoch 6 Step 95 ave_loss 0.0001\n",
            "Epoch 6 Step 127 ave_loss 0.0001\n",
            "Epoch 6 Step 159 ave_loss 0.0001\n",
            "Epoch 6 Step 191 ave_loss 0.0000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.60      0.86      0.70       409\n",
            "          3C       0.81      0.91      0.86       367\n",
            "          4C       1.00      0.73      0.84       831\n",
            "\n",
            "    accuracy                           0.80      1607\n",
            "   macro avg       0.80      0.83      0.80      1607\n",
            "weighted avg       0.85      0.80      0.81      1607\n",
            "\n",
            "Epoch 7 Step 31 ave_loss 0.0000\n",
            "Epoch 7 Step 63 ave_loss 0.0001\n",
            "Epoch 7 Step 95 ave_loss 0.0001\n",
            "Epoch 7 Step 127 ave_loss 0.0001\n",
            "Epoch 7 Step 159 ave_loss 0.0001\n",
            "Epoch 7 Step 191 ave_loss 0.0001\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.60      0.86      0.70       409\n",
            "          3C       0.81      0.91      0.86       367\n",
            "          4C       1.00      0.73      0.84       831\n",
            "\n",
            "    accuracy                           0.80      1607\n",
            "   macro avg       0.80      0.83      0.80      1607\n",
            "weighted avg       0.85      0.80      0.81      1607\n",
            "\n",
            "Epoch 8 Step 31 ave_loss 0.0001\n",
            "Epoch 8 Step 63 ave_loss 0.0001\n",
            "Epoch 8 Step 95 ave_loss 0.0001\n",
            "Epoch 8 Step 127 ave_loss 0.0001\n",
            "Epoch 8 Step 159 ave_loss 0.0001\n",
            "Epoch 8 Step 191 ave_loss 0.0001\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.59      0.86      0.70       409\n",
            "          3C       0.81      0.91      0.86       367\n",
            "          4C       1.00      0.73      0.84       831\n",
            "\n",
            "    accuracy                           0.80      1607\n",
            "   macro avg       0.80      0.83      0.80      1607\n",
            "weighted avg       0.85      0.80      0.81      1607\n",
            "\n",
            "Epoch 9 Step 31 ave_loss 0.0001\n",
            "Epoch 9 Step 63 ave_loss 0.0001\n",
            "Epoch 9 Step 95 ave_loss 0.0001\n",
            "Epoch 9 Step 127 ave_loss 0.0001\n",
            "Epoch 9 Step 159 ave_loss 0.0001\n",
            "Epoch 9 Step 191 ave_loss 0.0001\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.60      0.85      0.70       409\n",
            "          3C       0.80      0.92      0.86       367\n",
            "          4C       1.00      0.73      0.84       831\n",
            "\n",
            "    accuracy                           0.80      1607\n",
            "   macro avg       0.80      0.83      0.80      1607\n",
            "weighted avg       0.85      0.80      0.81      1607\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNE4wYwxn9vg"
      },
      "source": [
        "main('vgg19')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzVVMNp-oBRg"
      },
      "source": [
        "main('resnet50')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}